<!DOCTYPE html>
<html>
<body style="font-size:20px;">
<p><a href="#C26">WELCOME</a></p>
<p><a href="#C27">AZURE first project</a></p>
<p><a href="#C28">AWS</a></p>
<p><a href="#C29">project2 on AWS</a></p>
<p><a href="#C30">Java exp</a></p>
<p><a href="#C1">DEVOPS</a></p>
<p><a href="#C2">PCF</a></p>
<p><a href="#C3">Open stack</a></p>
<p><a href="#C4">open shift</a></p>
<p><a href="#C5">Docker</a></p>
<p><a href="#C6">Kubernetis</a></p>
<p><a href="#C7">Orchestration</a></p>
<p><a href="#C8">Jenkins</a></p>
<p><a href="#C9">Chef</a></p>
<p><a href="#C10">puppet</a></p>
<p><a href="#C11">Ansible</a></p>
<p><a href="#C12">Jenkins</a></p>
<p><a href="#C13">Maven</a></p>
<p><a href="#C14">GIT</a></p>
<p><a href="#C15">Linux ADmin</a></p>
<p><a href="#C16">Application servers</a></p>
<p><a href="#C17">Agile</a></p>
<p><a href="#C18">Migration</a></p>
<p><a href="#C19">Teraform</a></p>
<p><a href="#C20">ELK Nagios splunk</a></p>
<p><a href="#C21">SCM tools</a></p>
<p><a href="#C22">AEM</a></p>
<p><a href="#C23">RED HAT</a></p>
<p><a href="#C24">LINUX</a></p>
<p><a href="#C25">SPLUNK</a></p>

<h2 id="C26">Welcome</h2>
<p>Hi this is Vikas; I am certified aws developer associate with Over 7+ years of experience in IT industry. I started my career as a linux system admin and later got experience in continuous integrations and continuous deployment and configuration management to build and maintain the required infrastructure and now working as Sr devops cloud engineer.</p>

<h2 id="C27">AZURE First project</h2>
<p>coming to my day to day role involves. working 70% on cloud customizing and private sharing centers that are on AZURE up to 1000 instances.working with deploying docker instances on AZURE platforms

In my current project we are team of 7 people. I am one of the lead devops engineer and a cloud architect. we have 2 other developer 1 QA, 1 release manager and 1 product owner.
To advance microservice adaption our Azure team had to clearly describe the characteristics of our microservices, how they compared with existing APIs, and had to develop several POC to showcase the value proposition. Dealt with storages like Blob (Page and Block), SQL Azure for making it possible for every microservice to have its own database. Azure Resource Manager template – JSON to define one or more resources to deploy to a resource group. I was responsible for adding accounts and resource partners, mapping partner claims, adding and configuring account stores, and identifying and configuring applications using SSO and multifactor authentication and recently started working with kubernetes for docker containerization. also have good experience in deploying kubernetes on centOS7 platform. and in current project I am working on continuous integration, orchestration tools. primarily working with chef, puppet for configuration management tools and wrote many custom cookbooks using ruby scripts. Also has good experience in using troposphere for creating custom cloud formation templates. Lastly git is the version control we are using, continuous integration via jenkins. Monitoring vise i have good experience using splunk and ELK. 60% on windows and 40 % on linux environments.
Have good knowledge of azure various paas , iaas and Saas services provided by azure. Worked in two different modules which are azure resource manager and azure service management. Created virtual machines, created serverless API using azure functions, Azure service fabric to deploy, manage scalable microservices and containers, blobs for storage and mainly responsible for connecting azure to on-premise data center using expressroute. And built resource groups in windows azure and restricted access to resource groups using role based access controls. To automate the infrastructure used azure CLI and involved in accessing subscription using powershell. To monitor and troubleshoot azure resources I have used azure app insights. </p>

<h2 id="C28">AWS</h2>
<p>I have experience in multiple cloud environments and services like Aws, Microsoft azure, OpenStack.
when coming to aws I am certified and developer associate. I have hands on experience on AWS, various Platform as a Service components and Services provided by AWS. I Architected Applications for High Availability and fault tolerant. Implemented Security and Best practices in AWS. Experience in migrating various data center applications, services and databases to cloud. Designed Capacity planning and Auto scaling etc. I have used the boto3 sdk of aws it’s a python based scripting for writing all Automation. Experience in using various services include standard services like EC2, S3, EBS, SNS, SQS, Cloud Formation, Cloud Front, RDS, DynamoDB, Cloud Watch, ELB, Auto scaling to more recent services like Lambda and OpsWork with chef configuration. I have used AWS CLI as well.
</p>

<h2 id="C29">project2 on AWS</h2>
<p>coming to my day to day role involves. working 70% on cloud customizing and private sharing centers that are on aws up to 1000 instances. working with s3 deploying docker instances on ec2 platforms
when coming to my second project, I performed the role of a sr. DevOps Cloud engineer where I was Involved in Designing, installing, automating, administering, and optimize AWS solutions and components to ensure business continuity. Built and configured a virtual data center in the AWS cloud to support Enterprise Data Warehouse
Involved in Designing and Deploying multitude of applications utilizing almost all the AWS Stack including EC2, Route 53, Cloud Formation, RedShift, Cloud Front, Cloud watch, IAM, DynamoDB with NoSQL.
Launched Ec2 instances by using amazon machine images and configured them with custom operating systems and packages. And authored python script in could formation templates to configure auto scaling and elastic load balancing, automating the process.
Worked on Docker container services and implemented Kubernetes for orchestration of the containers. Implemented Master-minion’s architecture. 
created many clusters using Kubernetes. And I worked on creating many pods. I also wrote Yaml files to creates pods, replication controllers, services, deployments, labels, health checks, ingress. I managed the
 load balances using ingress. And I worked on our company Aws docker containers using kops. I have knowledge on web UI of Kubernetes, but I prefer command line interface than webUI. 
Designed Jenkins build pipelines for automation and continuous integration. And architected Jenkins Clusters for High Availability, familiar with server-side things like backups, integrate Jenkins with various tools like Jira, Testing frameworks like Junit, Mockito, SoapUI, Cucumber, Code Quality tools like Sonar, HP Fortify etc.
Implemented Chef for Infrastructure as a code or configuration management and deployment automation on application builds using client-server architecture.
I have written many Cookbooks and Recipes that would do from simple things like create files, directories, users and passwords, env variables etc to more advanced things like Deployments and Automation etc. I have thorough understanding of Roles, Envs, Data bags, Encryption with Vault, Chef Client and Server integration, Attributes, ERB templates etc.
Implemented Nagios and Splunk monitoring solution for mission critical servers. Integrated Splunk with AWS deployment using Puppet to collect data from all database server systems into Splunk.
</p>

<h2>Java exp</h2>
<p>I have written custom plugins in Jenkins and Sonar etc utilizing REST APIs and plugin interfaces. In general, I am familiar with java coding but not strong in it. I have experience doing DevOps, Build, Release of Java based applications.</p>

<h2 id="C1">DEVOPS</h2>
<p>I am more of a Sys Admin with lot of scripting and Automation experience
In my current role, I am acting as Sr DevOps Cloud Engineer, Automate Infra provisioning in Cloud, Wrote Automation scripts in Infra Config languages like Chef, Puppet, Deploy/run apps and services in Container Technologies like Docker, Help Automate and day to day support of various things in CI Pipelines for various Apps, Strong knowledge on CI tools like Jenkins, Build Automation tools like maven, gradle etc, Version control systems like Git etc…
 
(In your opinion, what is main thing about DevOps.)
Devops is both a company culture and set of Tools to automate software releases. Main thing is to use modern technologies like cloud, infra languages like puppet and chef and make sure to automate all tasks as part of a release.
 
Windows Azure Portal: To run an application, a developer accesses the Windows Azure portal through her Web browser, signing in with a Windows Live ID. She then chooses whether to create a hosting account for running applications, a storage account for storing data, or both
</p>

<h2 id="C2">PCF</h2>
<p>Designed, deployed, & integrated Pivotal Cloud Foundry on Azure.
Created bind user defined and built-in services in Pivotal Cloud Foundry (PCF)
summary point PCF
Knowledge and Hands-on experience in configuring pcf components OAuth2 server and login server, Cloud Controller, nsync, BBS, Cell Rep, Blobstore, Diego Cell
Experienced with working OAuth2 server & login server to provide identity management.
Configured deployment of applications using Cloud Controller.
Configured nsync, BBS and Cell Rep to keep applications running.
Worked with Blobstore for storing and managing Application code packages, Build packs.
Managed the lifecycle of containers and processes running using Diego cell rep.
</p>

<h2 id="C3">open stACK</h2>
<p>Used OpenStack APIs to launch server instances, created images, assign metadata to instances and images, create storage containers and objects, and complete other actions in OpenStack cloud.
OpenStack is a cloud platform by Rackspace written in python. It is used mainly for IAAS.
I have used OpenStack in my previous project for deploying an application which was based on IAAS.
Built this application based on Python and MySQL.
I have deployed the application into open stack and integrated it with chef repos.
Implemented automated local user provisioning in instances created in Openstack cloud.
Implemented horizontal auto-scaling using OpenStack components such as Heat, Ceilometer.
Experienced in working with OpenStack object store Swift to store lots of data efficiently, safely, and cheaply and storing unstructured data that can grow without bound.
Delivered Network-as-a-service in virtual compute environments using OpenStack Neutron.
Experience in working with Openstack Glance Rest API for getting VM image metadata and retreiver of actual image.


Experience in working with Openstack services such as Novaand glance, Swift, Keystone,Heatand  Ceilometer.
</p>

<h2 id="C4">open shift</h2>
<p>Build Docker images; create build and deployment configurations to deploy applications on Openshift. 
Patch, upgrade and scale Openshift environment. 
Use of Docker and Openshift to manage micro services for development and testing. 
</p>

<h2 id="C5">Docker</h2>
<p>I am very familiar and hands-on with Docker. I installed and configured a private docker register for our dev teams to push and pull containers. Familiar with all docker commands, wrote docker files to run applications, familiar with other advanced tools like Compose, Swarm, Weave. Container scheduling frameworks like Kubernetes(K8), Mesos etc.
Based on Applications their architecture and type, we can write docker files to create and run applications in docker containers. I have used Docker Compose/Swarm to create and run Cluster of containers, use Weave to configure Networking between them and deploy a n-tier application on them.
</p>

<h2 id="C6">Kubernatis</h2>
<p>I have good hands on experience with installing and configuration of Kubernetes and cluster them.

I have integrated Kubernetes with n/w, storage of security to provide a comprehensive infrastructure.
and also orchestrated the Kubernetes containers across the multiple hosts.
Created Clusters using Kubernetes kubectl and worked on creating many pods, replication controllers,
 services, deployments, labels, health checks and ingress by writing Yaml files.
Used Kubernetes to manage containerized applications using its nodes, ConfigMaps, selector, Services and deployed application containers as Pods.
Implemented Jenkins and built pipelines to drive all microservice builds out to the Docker registry and then deployed to Kubernetes. 
Executed a Kubernetes POC(proof 0f concept)  to demonstrate the technical viability of container orchestration.
Managed Kubernetes charts using Helm. Created reproducible builds of the Kubernetes applications, managed Kubernetes manifest files and managed releases of Helm packages.
Worked with containerization tools, can implement transition to Docker and develop distributed cloud system using Kubernetes.
Handled large volumes of containers with Docker Swarm, Kubernetes, and Mesos.
To spin  up a cluster in aws we use KOPS.
We use Minicube it runs a single node kubernetes cluster inside a linux VM's.</p>

<h2 id="C7">Orchestration</h2>
<p>Container Orchestration refers to the automated arrangement, coordination, and management of software containers.
Docker with Jenkins CI:
Docker containers can be used to run the applications or services that we are trying to test as part of our CI pipeline. We write scripts that would create containers on Jenkins slaves, deploy and run the build created during build on them
Java Experience:
I have written custom plugins in Jenkins and Sonar etc utilizing REST APIs and plugin interfaces. In general, I am familiar with java coding but not strong in it. I have experience doing DevOps, Build, Release of Java based applications.
</p>

<h2 id="C8">Jenkins</h2>
<p>I have experience in designing Jenkins build pipelines for automation and continuous integration. Very good understanding and implementation experience of CI/CD, Automation Programmatically creating Jenkins jobs and pipelines, solving various problems with plugins, Architecting Jenkins Clusters for High Availability, familiar with server-side things like backups, integrate Jenkins with various tools like Jira, Testing frameworks like Junit, Mockito, SoapUI, Cucumber, Code Quality tools like Sonar, HP Fortify etc.
 
(I have used both Chef and Puppet.)
Both are Infrastructure as a code or configuration management and deployment automation on application builds.
 
</p>

<h2 id="C9">Chef</h2>
<p>Chef Client - Version 13.3.42
Chef Client - Version 12.16.9
 
Chef is a Configuration Management tool i used it mainly to manage Infrastructure as Code (IaC). with this automation platform that configures and manages the infrastructure whether it is on-premises or in the cloud. It turns your infrastructure as code i.e. your computing environment has some of the same attributes as your application; Your infrastructure is version-able, repeatable, and testable. You only need to tell Chef what the desired configuration should be, not how to achieve it. Like Puppet which has a Master-Slave architecture, even Chef has a Client-Server architecture. But Chef has an extra component called Workstation.
I have had an interesting work experience with Chef. I used it when I was first introduced to DevOps in the US.  It was where I worked on creating cookbooks and authored recipes to provision the servers with required infrastructure. I configured, maintained and supported Chef environment on 1000+ Servers. I composed several Chef Cookbooks and Recipes to provision several pre-prod environments. Launched fully Chef configured and build management system to deploy servers with the proper configuration on a per role and per environment basis.
</p>

<h2 id="C10">Puppet</h2>
<p>Puppet version 4.1
puppet is used as  a configuration management tool which runs on Linux and Unix system. It includes a custom declarative language that is used to describe system configuration and their state. It simplifies various system administration tasks.
I have around 5 years of experience using Puppet under configuration management for deploying, configuring and managing servers. I have good experience with Puppet’s objects like Puppet resource, Puppet Class, Puppet Manifests, Puppet Modules, Puppet Forge.  We used Puppet for system configuration where we updated configuration from the master server on .pp file and pushed it to multiple nodes or clients. I was involved in deploying web applications using puppet by developing the manifests to meet the project requirements. The information was pulled and updated and had patch cycles a couple of time in a week. We made sure that the configuration along with all the environments (dev, test, prod) is the same and that there are not any technical issues.
</p>

<h2 id="C11">Ansible</h2>
<p>Ansible is an open source automation platform. You can use Ansible to automate three types of tasks:
Provisioning: Set up the various servers you need in your infrastructure.
Configuration management: Change the configuration of an application, OS, or device; start and stop services; install or update applications; implement a security policy; or perform a wide variety of other configuration tasks.
Application deployment: Make DevOps easier by automating the deployment of internally developed applications to your production systems.
I have pretty good experience using Ansible, through which I got to write many playbooks, modules and roles using YAML script and used them in AWS environments. I am experienced in configuring the Ansible Tower to automate repetitive tasks and quick deployments for the critical applications, also to manage Multiple Nodes and Manage Inventory for different Environments. Implemented SSH Keys to create password-less SSH connection on various servers. Basically, I used Ansible to provide infrastructure on servers, installed packages, required SDKs, etc. 

saltstack
It IS a Quickly scalable, very resilient and efficient because of multi-master capability
I Used minions because it offers more options and flexibility than Ansible
There is no GUI for it currently under development)
i have used Python and PyDSL for scripting
We have used Minions for efficient communication for large scale deployment
Minions not as efficient as agent-less communication for small-scale deployment
</p>

<h2 id="C12">Jenkins</h2>
<p>Jenkins is an open source automation tool written in Java with plugins built for Continuous Integration purpose. Jenkins is used to build and test your software projects continuously making it easier for developers to integrate changes to the project, and making it easier for users to obtain a fresh build. It also allows you to continuously deliver your software by integrating with many testing and deployment technologies.
In Jenkins, I architected build pipelines to retrieve code, compile applications, perform tests and push build artifacts to Nexus and implemented Master/Slave architecture. I am experienced in maintaining CI environments with build automation tools like Jenkins. I extensively used Jenkins to streamline CI/CD process. I configured Jenkins jobs for building the Java based code and deployed it to various test servers and production environments. Through Jenkins, we automated webserver content deployments via shell scripts, we also migrated to Jenkins from Hudson for Continuous Integration and deployment into Tomcat Application Server.
</p>

<h2 id="C13">Maven</h2>
<p>Experienced in invoking maven top level goals such as compile, test, package, deploy, install and clean.
Configured POM file to manage plugins such as build plugins and Reporting plugins  and resolve dependencies required for project.
Defined Profiles in POM file to provide different build results to different environments such as development and testing.
Created maven projects with different archetypes such as quickstart and webapp.
 
Maven is a build automation framework provided by apache. It is used for continuous deployment, building and merging Java projects.
· Used maven to build java projects and store them in XML files.
· Create artifacts like jar, war and ear from source code in the java projects.
· Defined project related dependencies and plugins in maven pom.xml files.
· Integrated maven with GIT to deploy project related tags.
· Integrated Maven with Jenkins and Hudson to schedule the builds.
· Maven Managed project dependencies in maven by creating parent child relationships among projects.
Installed Ant build tool and specified dependency jars in Ant path required for running build process.
Defined Ant targets, tasks such as Archive tasks, file tasks and properties in project for performing build process.
Monitored Ant build process using Listeners such as Log4JListener and Loggers such as Default Logger, Mail Logger.
</p>

<h2 id="C14">GIT</h2>
<p>Git is a free, open source distributed version control system tool designed to handle everything from small to very large projects with speed and efficiency. Git has the functionality, performance, security and flexibility that most teams and individual developers need.
I maintained GIT source code repository and local mirrors, performed branching, tagging, merging and maintenance tasks for windows host and Mac builds. Coordinate/assist developers with establishing and applying appropriate branching, labeling/naming conventions using GIT source control. Configured and administered GitHub enterprise, also, I handled migrations from SVN to GitHub.
</p>

<h2 id="C15">linux admin</h2>
<p>I have sysadmin level knowledge in Linux, I worked on various flavours like Centos, Red hat enterprise linux, Ubuntu, Solaris, Debian, fedora distributions etc. I have done a lot of Automation, Shell scripting and automate deployments of various applications in Linux.
I have extensive experience in yum and rpm package managers to install and update the packages and patching them whenever required. I am more of a command line guy and lots of experience in implementing commands. I can troubleshoot and solved network issues and solved why applications are down. I have also worked on creating VMs, Networking, Storage services, Security hardening etc.
</p>

<h2 id="C16">application servers</h2>
<p>My experience is mostly around Deployment and running apps in various Application Servers. I have used Web Logic, Web Sphere, JBoss etc. I wrote various scripts using various scripting interfaces like WLST (Web Logic), Python scripting on Web Sphere to run applications in standalone or clustered mode. As part of deployment, I created more resources like JDBS, JMS, JMX etc for applications etc.
I have also used Web Servers like Apache, Tomcat, Nginx etc.
 
Version control tools: git,github,subversion
Scripting langugaes: shell,ruby,python,
Build automation tools: ant,maven,gradle
Ci tools: Jenkins,baboo, hudson
Config manangment tools: puppet,chef,asbile
Monitoring tools: Nagios,splunk,cloud watch
Os: windows,linux
App servers: jboss,weblogic,web sphere
Web servers: apache http,tomcat, nginx
Container tools: docker, kubernertes
Orchestration tools: kubernetes, mesos, docker swarm
Virtualization tools:vm ware, virtual box, oracle vb
Clouds: aws, openstack, azure
Databases: amazon aurora, my sql,rds, dynamo db no sql ,mangodb
Patching: kernel, security to update and data recovery
</p>

<h2 id="C17">Agile</h2>
<p>Agile software development refers to a group of software development methodologies based on iterative development, where requirements and solutions evolve through collaboration between self-organizing cross-functional teams. Agile methods or Agile processes generally promote a disciplined project management process that encourages frequent inspection and adaptation, a leadership philosophy that encourages teamwork, self-organization and accountability, a set of engineering best practices intended to allow for rapid delivery of high-quality software, and a business approach that aligns development with customer needs and company goals. Agile development refers to any development process that is aligned with the concepts of the Agile Manifesto. The Manifesto was developed by a group fourteen leading figures in the software industry, and reflects their experience of what approaches do and do not work for software development. Read more about the Agile Manifesto.</p>

<h2 id="C18">Migration experiance</h2>
<p>I did planning and execution of applications on our existing infrastructure in the data center. Identified what type of services can be used and what configuration to better our servers. Right from identifying Availability Zones and regions and eventually migrating these applications by programmatically creating temporary infrastructure and once it is created, we configured Chef to deploy these applications to be running in cloud. We made sure Chef Recipes worked against AWS setup and monitored how the code worked on the master branch. We did standardized pipeline with SDLC process and orchestration through Jenkins, we also used Nexus server for artifacts and wrote scripts for deploying. I have done deployments in production environment through Blue-Green Deployment technique to decrease risks and downtime; we migrated around 10tb of data into AWS cloud. Basically, we used AWS Snowball service for migrating Oracle Database Enterprise into AWS Aurora.</p>

<h2 id="C19">terraform</h2>
<p>Experienced in writhing terra templates that can spin up infra for infrastructure for multi-tier applications and created and provisioned boot strapped software demo on cloud with terraform and deployed infrastructure in multi cloud environment to deploy a fault tolerant application.
Terraform works just like cloud formation but it is available for all cloud platforms and not designed to a specific cloud platform like cloud formation to AWS. It is open to many scripting languages like python, Bash, JSON, and HCL.
· I am currently working with terraform to create templates for environments using HCL scripting.
· Using terraform to automate the deployment process onto azure.
· Converted cloud formation JSON templates to terraform.
· Also performed AWS infrastructure automations with terraform.
SonarQube
Worked with SonarQube for continuous inspection applications in our environment and detect newly raised issues like bugs and security issues. In my project I checked in the application code to GITHUB and used SonarQube for continuous inspection by linking SonarQube to NetBeans Platform.
</p>

<h2 id="C20">ELK Nagios splunk</h2>
<p>In Elasticsearch I was mainly involved with index management, shard allocation and snapshot the data to a different repository for disaster recovery, log stash for data parsing extracted values from text messages and dealt with the increase of load on the log stash servers and to restrict access to certain info in Kibana I have used role based access.
Splunk
Monitoring tool. Performs searching, analysis, has a dashboard, based on saas and on premises setup. Has plugins. Can input any data type. It has customer support and does documentation.
· https://intellipaat.com/tutorial/splunk-tutorial/
· I have good experience with Monitoring tools like Splunk. By using Splunk we can efficiently analyze, store and process the data. Splunk stores data in index form so its require separate database. It automatically searches the useful data without manual effort. The biggest advantage is it converts log data into graphs so that we can simplify the analysis, reports and troubleshooting.
 
· The biggest challenge with big data is effective searching and here is the quick guide navigating you through proper searching channels such that you master the text-in-search techniques using Splunk.
· A detailed operational understanding on Search Processing Language (SPL) with examples is a great learning benefit you can achieve.
· The tutorial covers quick and easy-to-understand tips and tricks to enrich your data for processing purposes.
· Monitoring, Alerting, grouping events and gaining expert-level understanding on Lookup tables are some of the excellent coaching topics of this tutorial.

NAGIOS:
Configured Nagios and implement Nagios compliant scripts.
Experienced in configuring NDO Utils components such as NDOMOD Event Broker Module, LOG2NDO, FILE2SOCK, NDO2DB.
Configured Stalking in Nagios for logging purposes. Integrated passive checks with Nagios to monitor services that are asynchronous in nature.
Integrated Event Handlers such as logging event information to a database and restarting faild service with Nagios to proactively fix problems.
Developed splunk queries and dashboards targeted at understanding application performance and capacity analysis.
Experienced in planning and building Splunk Cluster environment with High Availability resources.
Designed, supported and maintained Splunk cluster infrastructure in a highly available configuration.
Troubleshooted the Splunk performance, search polling, role mapping and log monitoring issues.
Wrote intelligent custom health checks using Nagios to reduce notification noise and automate service restarts.
Configured Nagios for continuous monitoring of applications in the production environment and enabled notifications via emails.

splunk
Splunk is a software platform to search, analyze and visualize the machine-generated data gathered from the websites, applications, sensors, devices etc. which make up your IT infrastructure and business.
</p>

<h2 id="C21">SCM tools</h2>
<p>Experienced in branching, tagging and versioning control using Github.
Supported development team on resolving git branching and merging issues.
Worked on Client-side hooks such as committing and merging, server-side hooks run on network operations.
Pulled requests with code review and comments using Bitbucket.
Created snippets using Bitbucket that allow developers to share code segments
Integrated SonarQube plugin to centralize the configuration of SonarQube server connection details in Jenkins global configuration.
Integrated Matrix Authorization Strategy Plugin with Jenkins to provide specific permissions for Jenkins jobs.
Defined environment variables within the Jenkinsfile useful for instructing scripts to configure builds to run in Jenkins.
Integrated parameterized trigger plugin to pass parameters between jobs during downstream/upstream Jenkins projects.
Worked on integration of pre-scm-buildstep plugin with Jenkins to specify tasks to be done before SCM polling.
worked on integration of Build-timeout plugin with Jenkins to specify timeout for running builds.
worked on integration of BuildCompleteAction Module with Bamboo to add custom asynchronous action after the build process has completed.
Integrated Notification Condition Module with Bamboo to add new notification condition.
Configured BuildPlanDefinition interface to encapsulate all information required to execute a Bamboo build plan.
Configured AdministrationConfiguration for storing Bamboo's system wide properties such as the default url, instance name, global vairables and System Properties.
</p>

<h2 id="C22">AEM</h2>
<p>Adobe experience management system.
· It is a web content management system for enterprise levels.
· It’s mainly used for building and managing website applications, mobile applications and forms.
· It is java- based platform and works on OS that support Adobe platform.
· Interactions with AEM is done on a web-browser.
Cloud Bees
It used to be a PAAS cloud provider but now focuses on providing open source continuous integration tool called Jenkins. Enterprises choose Cloud Bees to automate previously created manual jobs.
Packer
Open source tool for creating machine images on multiple platforms. Tool for creating AMIs, Virtual Box Images, Vagrant Boxes. It is like a template to generate images for multiple platforms.
Packer is an open source tool for creating identical machine images for multiple platforms from a single source configuration. Packer is lightweight, runs on every major operating system, and is highly performant, creating machine images for multiple platforms in parallel. Packer does not replace configuration management like Chef or Puppet. In fact, when building images, Packer can use tools like Chef or Puppet to install software onto the image. Its plugin we used in Jenkins
A machine image is a single static unit that contains a pre-configured operating system and installed software which is used to quickly create new running machines. Machine image formats change for each platform. Some examples include AMIs for EC2, VMDK/VMX files for VMware, OVF exports for VirtualBox, etc.
</p>

<h2 id="C23>red hat</h2>
<p>1. Redhat is free but it is charged for support (updates),Redhat is the corporate version based on the progress of that project, and it has slower releases, comes with support, and isn’t free.
2.  RedHat Satellite Minor Version Update from 6.2.11 to 6.2.13, Migration of RedHat Satellite 6.2.13 from RHEL 6.9 TO RHEL 7.4, Registering to Red hat Subscription Management (RHSM), Add necessary subscriptions, Review existing content Host Registrations, Register New Content hosts, selecting Repositories, setting Sync time, Freezing & Publishing New Versions of REPO Data, Refreshing Subscription Manager on Hosts, Security Errata and patch Maintenance of new and existing Servers, Resolving any dependencies, Linux Red Hat Subscription Manager maintenance and local repo setup.
3.  Managed/Configured RedHat Satellite server patch management system.
4. Creation of Volumes and file systems on SAN disks using Redhat LVM and VERITAS Volume Manager.
5. Activities include user administration, startup and shutdown scripts, crontabs, file system maintenance and backup scripting and automation using shell scripting (BASH, KSH) and Perl for RedHat Linux systems.
6. Performed Patching and upgrades (release), on stand-alone servers (using single user mode), and live upgrade of servers in production and Linux using YUM update / RPM manager from repository or Redhat subscription management service (RHSM).
7. Creating new groups in RedHat Satellite server and adding servers to configured groups
8. Installed and configured RedHat Satellite server for package transfer across servers, to store client's system profile and to control other servers in the network.
9.  I am also experienced on Raspberry pi on which I am  installed centos in that particular centos maintain kubernetes clusters as well as deploying those clusters to different remote servers.
 
</p>

<h2 id="C24">linux</h2>
<p>With my experience as a Linux administrator and a system administrator, I am capable of handling admin level operations and duties on Linux. I worked with Linux distributions like RedHat Linux, CentOS, Ubuntu, Debian, SUSE and Solaris.
· Performed regular heath checks, Taken backups, maintained firewalls and monitored Linux systems using Nagios software.
· I troubleshooted networking issues, implemented protocols and provided 24x7 on-call support for Linux servers.
· Installed RedHat Linux servers locally and over networks using Kickstart.
· Setup Samba servers for various Linux systems and enabled communication of windows client with Linux system.
· I have worked with VMware virtualization systems and migrated applications from Linux to windows.    
· I have performed LVM to manage volumes, expanded file system spaces and created NFS mounts. 
 
</p>

<h2 id="C25">splunk</h2>
<p>Monitoring tool. Performs searching, analysis, has a dashboard, based on saas and on premises setup. Has plugins. Can input any data type. It has customer support and does documentation.
ELK
Searching analysis and dashboard is only possible by integration. Based on saas and on premises setup. Has plugins and documentation. Any input data type is not available.
In our company we use elk to process data from log files coming from different applications, basically we are using Ubuntu server for java applications A cluster is identified by a unique name which by default is "elasticsearch". This name is important because a node can only be part of a cluster if the node is set up to join the cluster by its name. 
Make sure that you don’t reuse the same cluster names in different environments, otherwise you might end up with nodes joining the wrong cluster using the ELK stack to monitor the logs from some Spring Boot’s microservices, backup, monitor and configure another aspect like security on a cluster.
(In elk, created yaml files and used azure resource management templates, high level architecture storage, pod on nodes, cluster, need a data node where we can hold one or more data client node will handle incoming request and master node perform cluster management and operation side like maintaining and distributing the information around cluster connect to cluster with api, memory requirement)
</p>

<h2></h2>
<p></p>

<h2></h2>
<p></p>

<h2></h2>
<p></p>

<h2></h2>
<p></p>

<h2></h2>
<p></p>

</body>
</html>
